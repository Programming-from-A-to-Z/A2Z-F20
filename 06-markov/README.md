# Markov Chains

## Markov Chains

- ğŸ“• [Markov Chains](http://setosa.io/blog/2014/07/26/markov-chains/) by Victor Powell and Lewis Lehe
- ğŸš¨ [Markov Chain video tutorial Part 1](https://youtu.be/eGFJ8vugIWA)
- ğŸ¿ [Markov Chain video tutorial Part 2](https://youtu.be/9r8CmofnbAQ)
- ğŸ’» [Markov Chain p5.js code examples](https://editor.p5js.org/a2zitp/collections/WEXEPRHuE)
- ğŸ“š [N-Grams and Markov Chains by Allison Parrish](http://www.decontextualize.com/teaching/rwet/n-grams-and-markov-chains/)
- ğŸ“š [2016 Markov Chains notes from A2Z](https://shiffman.net/a2z/markov/)

### Markov Project References

- ğŸ¨ [ITP Course Generator by Allison Parrish](http://static.decontextualize.com/toys/next_semester)
- ğŸ¨ [WebTrigrams by Chris Harrison](http://www.chrisharrison.net/index.php/Visualizations/WebTrigrams)
- ğŸ“ˆ [Google N-Gram Viewer](https://books.google.com/ngrams), [google blog post about n-grams](http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html)
- ğŸ¨ [King James Programming](http://kingjamesprogramming.tumblr.com/)
- ğŸ¨ [Gnoetry](http://www.beardofbees.com/gnoetry.html)

## Reading

- ğŸ“• [What Can Machine Learning Teach Us About Ourselves?](https://medium.com/processing-foundation/what-can-machine-learning-teach-us-about-ourselves-65b268431890), Interview with Emily Martinez, ml5.js Fellow 2020
- ğŸ“• [The Subtext of a Black Corpus](https://medium.com/ml5js/the-subtext-of-a-black-corpus-4440de02eb32), In conversation with ITP research fellows Nikita Huggins & Ayodamola Okunseinde by Ashley Lewis

## Assignment

Use one of the [existing examples](https://editor.p5js.org/a2zitp/collections/WEXEPRHuE) to generate text with your own input data. Experiment with the "order" and "maximum" length variables. Try mixing multiple texts. Copy paste your favorite outputs from the browser and document in a blog post.

Emily Martinez proposes a [series of questions to ask related to working with a corpus of text data](#emily-martinez-questions). Reflect on these questions and how they played into your process working with source texts in your documentation post.

It is not required to write any new code for this assignment, however I'll include some ideas for further exploration below.

- Design a webpage that displays the output of a markov generator a la [Allison Parrish's ITP course creator](http://static.decontextualize.com/toys/next_semester).
- Create a bot that generates its output based on a markov chain.
- Use a markov chain on something other than text. Record your own sequence of daily habits. Try musical notes. Could colors or shapes be generated with a markov chain? What else? You can find examples for [musical markov chains](https://luisaph.github.io/the-code-of-music-2018/#Markov) from Luisa Pereira's [Code of Music materials](https://luisaph.github.io/the-code-of-music-2018/).
- Thinking back to [the word counting material](https://github.com/shiffman/A2Z-F20/tree/main/04-word-counting), visual n-gram frequencies and/or markov probabilities.

### Add your assignment below via Pull Request

_(Please note you are welcome to post under a pseudonym and/or password protect your published assignment. Here is some [helpful information on privacy options for an NYU blog](https://nyu.service-now.com/sp?id=kb_article&sysparm_article=KB0012245&sys_kb_id=b2ddc9da004aa1002a5d036a271e5f70&spa=1). Finally, if you prefer not to post your assignment at all here, you may email the submission.)_

- Name - [post title](post url)

## Emoji Key for Video Tutorials, Readings, and more

- ğŸš¨ Watch this video tutorial! (this is technical info needed for the examples). Of course if you alreaddy know this material, you can skip.
- ğŸ”¢ This is found in a group, maybe pick just one to check out!
- ğŸ¿ Additional video if you have a particular interest and want to do a deeper dive.
- ğŸ“• Required reading! Let's make sure we all have read this.
- ğŸ“š Optional additional reading for a deeper dive.
- ğŸ’» Code examples here!
- ğŸ“ˆ Class presentation slides
- ğŸ”— Extra reference material / link

## Emily Martinez Questions

- How can we be more intentional about what we build given the current limitations, problems, and constraints of ML algorithms?
- How do we prepare datasets and set up guidelines that protect the bodies of knowledge of our communities, that honors lineage, that upholds ethical frameworks rooted in shared, agreed-upon values?
- How do we work in consensual and respectful ways with texts by marginalized authors that are not as well-represented, and by virtue of that fact alone, much more likely to be misrepresented, misappropriated, or misunderstood if we are not careful?
- How well can we ensure that the essence of these texts doesnâ€™t dissolve into a word-soup that gets misconstrued?
- Given that so many of the existing â€œbig dataâ€ language models are trained with Western texts and proprietary datasets, what does it even mean to try to decolonize AI?
- Who do we entrust to do this work?
- How do we deal with credit and attribution of our new creations?
- How do we really do ethics with machine learning?
- How do we get through this whole list of concerns and still build AI that is fun, respectful, tender, pleasurable, kind?
